{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0bfef2-221e-4c01-9790-cc5290352909",
   "metadata": {},
   "source": [
    "# Rekurencyjne Sieci Neuronowe (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748086a5-e233-4dcd-bb45-efaafbeae7af",
   "metadata": {},
   "source": [
    "* 2025/2026, A. Kania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfd618-88e5-48e0-b7b8-abb8d1091409",
   "metadata": {},
   "source": [
    "Zagadnienia na dziś:\n",
    "- Rekurencyjne sieci neuronowe\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad46aec7-3c64-4b8d-946d-ede6e82d4abd",
   "metadata": {},
   "source": [
    "## Dane sekwencyjne\n",
    "\n",
    "Modele, którymi zajmowaliśmy się wcześniej zakładały konkretny kształt danych. Dla przykładu klasyczna sieć neuronowa fully-connected dla MNISTa zakładała, że na wejściu dostanie wektory rozmiaru 784 - dla wektorów o innej wymiarowości i innych obiektów model zwyczajnie nie będzie działać.\n",
    "\n",
    "Takie założenie bywa szczególnie niewygodne przy pracy z niektórymi typami danych, takimi jak:\n",
    "* językiem naturalny (słowa czy zdania mają zadanej z góry liczby znaków)\n",
    "* szeregi czasowe (dane giełdowe ciągną się właściwie w nieskończoność) \n",
    "* dźwięk (nagrania mogą być krótsze lub dłuższe).\n",
    "\n",
    "Do rozwiązania tego problemu służą rekuencyjne sieci neuronowe (*recurrent neural networks, RNNs*), które zapamiętują swój stan z poprzedniej iteracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88fe7efe-387b-43af-9ccf-c77e7db370cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bb15aa4-fb03-43f5-b43e-6c035f495956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, Optional, List\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "char_to_idx = {c: i for i, c in enumerate(all_letters)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "data = []\n",
    "targets = [] \n",
    "label_to_idx = {}\n",
    "\n",
    "def unicode_to__ascii(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
    "                                                                 and c in all_letters)\n",
    "\n",
    "def read_lines(filename: str) -> List[str]:\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicode_to__ascii(line) for line in lines]\n",
    "\n",
    "def letter_to_index(letter: str) -> int:\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "\n",
    "for label, file_name in enumerate(os.listdir(data_dir)):\n",
    "    \n",
    "    label_to_idx[label] = file_name.split('.')[0].lower()\n",
    "    \n",
    "    names = read_lines(os.path.join(data_dir, file_name))\n",
    "    data += names\n",
    "    targets += len(names) * [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db3dcc45-f4d8-4848-a067-25f107890855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Khoury'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e93c42d5-3f65-497e-833d-ccf8fef4e154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd23161f-0c0c-4748-b5b5-7a9907c3a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b0323-217c-46eb-8968-a99283c451a9",
   "metadata": {},
   "source": [
    "<h3> RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05002e51-a953-42ab-be27-1dd2a0668e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d46cd02d-e81b-419e-8fad-582d09b236b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_onehot(text):\n",
    "    \"\"\"Zamienia tekst na sekwencję wektorów one-hot.\"\"\"\n",
    "    seq = torch.zeros(len(text), n_letters)\n",
    "    for i, ch in enumerate(text):\n",
    "        seq[i, char_to_idx[ch]] = 1.0\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa6a14d4-3f5d-4ff8-97f4-546974263fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_onehot(\"ala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "110a3ae0-c275-4e50-acbb-3ad6cea880ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return text_to_onehot(self.data[idx]), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "#do obslugi sekwencji o roznej dlugosci\n",
    "def collate_fn(batch):\n",
    "    # batch = [(seq1, label1), (seq2, label2), ...]\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    padded = pad_sequence(sequences, batch_first=True)  # wymiar: (batch, max_len, vocab_size)\n",
    "    labels = torch.stack(labels)\n",
    "    return padded, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f483ee48-0e88-4b78-94cc-b156fc903a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(X))\n",
    "train_ind, test_ind = train_test_split(indices, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "train_targets = [y[i] for i in train_ind]\n",
    "test_targets = [y[i] for i in test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "517072f2-ca68-46da-9846-e69ca2adb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = len(train_targets) / counts\n",
    "weight = [weight_per_class[c] for c in train_targets]\n",
    "sampler = WeightedRandomSampler(weights=weight, num_samples=len(weight))\n",
    "\n",
    "train_dataset = ListDataset([X[i] for i in train_ind], train_targets)\n",
    "test_dataset = ListDataset([X[i] for i in test_ind], test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, sampler=sampler, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fdf1246-fec0-4ed2-80b3-9ce917c1154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, hidden = self.rnn(packed)\n",
    "        # hidden: (1, batch, hidden_size)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55451d59-c684-41a2-9141-29e2533f95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(y))\n",
    "model = RNNClassifier(input_size=n_letters, hidden_size=32, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3d5f811-01e6-4d24-8fd3-168e49956f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7955\n",
      "Epoch 2, Loss: 1.5811\n",
      "Epoch 3, Loss: 1.5504\n",
      "Epoch 4, Loss: 1.6250\n",
      "Epoch 5, Loss: 1.6921\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "178e16a9-1ce4-4305-bdcf-76df7ec711dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test results ===\n",
      "Top-1 accuracy: 42.32%\n",
      "Top-3 accuracy: 69.68%\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, loader, topk=(1,3)):\n",
    "    model.eval()\n",
    "    correct = {k: 0 for k in topk}\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            total += y_batch.size(0)\n",
    "            for k in topk:\n",
    "                _, pred = outputs.topk(k, dim=1)\n",
    "                correct[k] += (pred == y_batch.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "    for k in topk:\n",
    "        acc = 100 * correct[k] / total\n",
    "        print(f\"Top-{k} accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ee038-3ab0-4b5f-bb72-f2ae1cdf86af",
   "metadata": {},
   "source": [
    "<h3> LSTM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "752ef3f5-83dc-437f-98e4-0ca1e8632849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 1.5089\n",
      "Epoch 02 | Loss: 0.8308\n",
      "Epoch 03 | Loss: 0.6382\n",
      "Epoch 04 | Loss: 0.5455\n",
      "Epoch 05 | Loss: 0.5003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m outputs = model(X_batch, lengths)\n\u001b[32m     31\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m optimizer.step()\n\u001b[32m     34\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=(dropout if num_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.lstm(packed)\n",
    "        # hidden: (num_layers, batch, hidden_size)\n",
    "        out = self.dropout(hidden[-1])  # ostatnia warstwa, ostatni stan\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# === 7. Inicjalizacja ===\n",
    "num_classes = len(set(y))\n",
    "model = LSTMClassifier(input_size=n_letters, hidden_size=64, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# === 8. Trening ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# === 9. Ewaluacja ===\n",
    "def evaluate(model, loader, topk=(1,3)):\n",
    "    model.eval()\n",
    "    correct = {k: 0 for k in topk}\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            total += y_batch.size(0)\n",
    "            for k in topk:\n",
    "                _, pred = outputs.topk(k, dim=1)\n",
    "                correct[k] += (pred == y_batch.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "    for k in topk:\n",
    "        acc = 100 * correct[k] / total\n",
    "        print(f\"Top-{k} accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5e29159-286a-48f8-bdb8-9bf14c7c842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('antiinfective', 2412), ('antineoplastic', 1175), ('cns', 1149), ('cardio', 797)]\n",
      "  image_name            tags  \\\n",
      "1     pics/1   antiinfective   \n",
      "2     pics/2   antiinfective   \n",
      "3     pics/3  antineoplastic   \n",
      "5     pics/5             cns   \n",
      "6     pics/6             cns   \n",
      "\n",
      "                                              smiles                Col3  \n",
      "1  CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(N...   ['antiinfective']  \n",
      "2  CCCCC(C)C(=O)OC1C(C)C(CC)OC2(CC3CC(C/C=C(\\C)CC...   ['antiinfective']  \n",
      "3  COc1cc2c(c(OC)c1OC)-c1c(cc3c(c1OC)OCO3)C[C@H](...  ['antineoplastic']  \n",
      "5  CC(=O)OCC(=O)C1CCC2C3CCC4CC(O)CCC4(C)C3C(=O)CC12C             ['cns']  \n",
      "6                         CC(=O)Nc1nnc(S(N)(=O)=O)s1             ['cns']  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"all_chem_df.csv\", sep = \",\")\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df[\"tags\"]).most_common(4))\n",
    "\n",
    "df2 = df[df['tags'].isin(['antiinfective', 'antineoplastic', 'cns','cardio'])]\n",
    "print(df2.head())\n",
    "\n",
    "wszystkie = list(df2[\"smiles\"])\n",
    "\n",
    "zlaczone = \"\".join(list(df2[\"smiles\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "25e5f296-5ee5-4224-bc9d-698c9a71fc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCC[C@@]1(CCc2ccccc2)CC(O)=C([C@H](CC)c2cccc(NS(=O)(=O)c3ccc(C(F)(F)F)cn3)c2)C(=O)O1\n"
     ]
    }
   ],
   "source": [
    "X = wszystkie\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "beaeea58-1b11-402c-bda2-28823b20fb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "kategorie = {'antiinfective':0, 'antineoplastic':1, 'cns':2,'cardio':3}\n",
    "y = [kategorie[elem] for elem in df2[\"tags\"]]\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d0042eb-3ba9-4ee5-a67e-ac9643c93a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = \"\".join(list(set(zlaczone))) #wszystkie symbole w smiles\n",
    "n_letters = len(all_letters)\n",
    "char_to_idx = {c: i for i, c in enumerate(all_letters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "393c1e4f-9954-401b-a42d-548c81d7e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(X))\n",
    "train_ind, test_ind = train_test_split(indices, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "train_targets = [y[i] for i in train_ind]\n",
    "test_targets = [y[i] for i in test_ind]\n",
    "\n",
    "uni, counts = np.unique(train_targets, return_counts=True)\n",
    "weight_per_class = len(train_targets) / counts\n",
    "weight = [weight_per_class[c] for c in train_targets]\n",
    "sampler = WeightedRandomSampler(weights=weight, num_samples=len(weight))\n",
    "\n",
    "train_dataset = ListDataset([X[i] for i in train_ind], train_targets)\n",
    "test_dataset = ListDataset([X[i] for i in test_ind], test_targets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, sampler=sampler, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bab380dd-43a1-4c2b-bfa4-01a935b6f100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 1.3471\n",
      "Epoch 02 | Loss: 1.2586\n",
      "Epoch 03 | Loss: 1.1129\n",
      "Epoch 04 | Loss: 1.1046\n",
      "Epoch 05 | Loss: 0.9517\n",
      "\n",
      "=== Test results ===\n",
      "Top-1 accuracy: 60.54%\n",
      "Top-2 accuracy: 80.30%\n"
     ]
    }
   ],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=(dropout if num_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (hidden, cell) = self.lstm(packed)\n",
    "        # hidden: (num_layers, batch, hidden_size)\n",
    "        out = self.dropout(hidden[-1])  # ostatnia warstwa, ostatni stan\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# === 7. Inicjalizacja ===\n",
    "num_classes = len(set(y))\n",
    "model = LSTMClassifier(input_size=n_letters, hidden_size=64, num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "# === 8. Trening ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch, lengths in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch, lengths)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# === 9. Ewaluacja ===\n",
    "def evaluate(model, loader, topk=(1,2)):\n",
    "    model.eval()\n",
    "    correct = {k: 0 for k in topk}\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, lengths in loader:\n",
    "            outputs = model(X_batch, lengths)\n",
    "            total += y_batch.size(0)\n",
    "            for k in topk:\n",
    "                _, pred = outputs.topk(k, dim=1)\n",
    "                correct[k] += (pred == y_batch.view(-1, 1)).any(dim=1).sum().item()\n",
    "\n",
    "    for k in topk:\n",
    "        acc = 100 * correct[k] / total\n",
    "        print(f\"Top-{k} accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef2b2a-3e03-4a7f-b3ad-774b55bde9fe",
   "metadata": {},
   "source": [
    "<h4> Zadanie z przetwarzaniem sekwencji białkowych, pobawic sie roznymi tokenami i reprezentacjami (embedingmai):\n",
    "- np 1 aminokwas - 1 slowo - [hydrofoboossc, pI, gravy] <- embedding\n",
    "- sekwencjei nukleoptydowe - np 3 kolejne slowa, arbitralnie, embeddingi 64D\n",
    "\n",
    "powiedziec, ze jak tekst to tokenem moze byc 1 znak, 1 slowo a aczasem cos pomiędzy i zaprosic na kurs z NLP, powiedziec ze tam jest tylko o przetwarzaniu tekstu\n",
    "\n",
    "Pokazac: https://platform.openai.com/tokenizer I like bioinformatics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933b997-b0a0-4741-9822-424273d7eea7",
   "metadata": {},
   "source": [
    "Zadanie: jakis gotowy model Transformer do dotrenowania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272dc550-fa69-42b5-8ae0-b73682c6add6",
   "metadata": {},
   "source": [
    "https://aisingapore.org/animated-rnn-lstm-gru/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f41ddc-edb5-4050-8785-a311c19db6ad",
   "metadata": {},
   "source": [
    "Obrazek rnn i gify rnn pokazac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c1b17-5162-4a6c-b556-90ab2639ee3b",
   "metadata": {},
   "source": [
    "<h4> Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1809af28-e94b-4459-af30-a265e90f5adf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      2\u001b[39m classifier = pipeline(\u001b[33m\"\u001b[39m\u001b[33msentiment-analysis\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e219320-0b13-4fbd-b96d-195db7385425",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/6914d5ba-217c-832c-bcad-69048cfe4499"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7392f8-17c6-4905-af14-a023f224dfc1",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/69199e9e-2b1c-8328-b7e5-3ff5dc5e0201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1de2a5-54f5-4b5d-a5cd-902242d9af5b",
   "metadata": {},
   "source": [
    "<h4> Modele z huggin face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe4f389-f473-487f-9830-2fac32fca8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3314fc88-9ffe-41a8-a6e3-d199fec04821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0011, 0.0021, 0.1344, 0.8516, 0.0108]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "texts = [\"a dog\", \"a happy dog\", \"a cat\", \"a happy cat\", \"grass\"]\n",
    "\n",
    "image = Image.open(\"cat.jpg\")\n",
    "\n",
    "inputs = processor(\n",
    "    text=texts,\n",
    "    images=image,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "probs = outputs.logits_per_image.softmax(dim=1)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa9e0e-277d-48ab-ae1d-d3c7470136f4",
   "metadata": {},
   "source": [
    "Tekst -> Obrazek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3268fc0f-0fdd-441c-9be2-d103b984c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b53d9b9-9896-461d-911e-294e09ff3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Couldn't connect to the Hub: 401 Client Error. (Request ID: Root=1-6919a867-20683b056c92be164cee9e66;5aeda580-b9c4-4836-8ec2-777d46848337)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/api/models/CompVis/ss.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password..\n",
      "Will try to load from local cache.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot load model CompVis/ss: model is not cached locally and an error occurred while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/CompVis/ss",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1483\u001b[39m, in \u001b[36mDiffusionPipeline.download\u001b[39m\u001b[34m(cls, pretrained_model_name, **kwargs)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1483\u001b[39m     info = \u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPError, OfflineModeIsEnabled, requests.ConnectionError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\hf_api.py:2661\u001b[39m, in \u001b[36mHfApi.model_info\u001b[39m\u001b[34m(self, repo_id, revision, timeout, securityStatus, files_metadata, expand, token)\u001b[39m\n\u001b[32m   2660\u001b[39m r = get_session().get(path, headers=headers, timeout=timeout, params=params)\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2662\u001b[39m data = r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:452\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    443\u001b[39m     message = (\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    451\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-6919a867-20683b056c92be164cee9e66;5aeda580-b9c4-4836-8ec2-777d46848337)\n\nRepository Not Found for url: https://huggingface.co/api/models/CompVis/ss.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pipe = \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCompVis/ss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:833\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    829\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    830\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe provided pretrained_model_name_or_path \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    831\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     cached_folder = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    852\u001b[39m     cached_folder = pretrained_model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\bioinformatyka\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1677\u001b[39m, in \u001b[36mDiffusionPipeline.download\u001b[39m\u001b[34m(cls, pretrained_model_name, **kwargs)\u001b[39m\n\u001b[32m   1674\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1676\u001b[39m     \u001b[38;5;66;03m# 2. we forced `local_files_only=True` when `model_info` failed\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1678\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: model is not cached locally and an error occurred\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1679\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1680\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1681\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_info_call_error\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Cannot load model CompVis/ss: model is not cached locally and an error occurred while trying to fetch metadata from the Hub. Please check out the root cause in the stacktrace above."
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835f8bc0-4fd7-4184-8978-a748ccac9b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery happy dog\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#ok 15/20 min sie generuje\u001b[39;00m\n\u001b[0;32m      2\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhappy_dog.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "image = pipe(\"very happy dog\").images[0] #ok 15/20 min sie generuje\n",
    "image.save(\"happy_dog.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
